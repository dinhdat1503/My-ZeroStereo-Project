import os
import glob
import sys
from pyspark.sql import SparkSession

# --- C·∫§U H√åNH CHO KAGGLE ---
# L∆∞u √Ω: B·∫°n c·∫ßn thay ƒë√∫ng t√™n dataset c·ªßa b·∫°n ·ªü d√≤ng d∆∞·ªõi
INPUT_DIR = "/kaggle/input/bdd100k-dataset/bdd100k/images/100k/train" 
OUTPUT_DIR = "/kaggle/working/spark_output"

# H√†m Map (X·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠)
def process_partition(iterator):
    import torch # Import b√™n trong h√†m ƒë·ªÉ tr√°nh l·ªói Spark
    results = []
    
    # Gi·∫£ l·∫≠p check GPU tr√™n Worker
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    for image_path in iterator:
        filename = os.path.basename(image_path)
        try:
            # --- CH·ªñ N√ÄY G·ªåI MODEL C·ª¶A B·∫†N ---
            # V√≠ d·ª•: model.predict(image_path)
            # ·ªû ƒë√¢y m√¨nh ch·ªâ gi·∫£ l·∫≠p t·∫°o file k·∫øt qu·∫£ r·ªóng
            
            # Ghi file k·∫øt qu·∫£ (Demo)
            os.makedirs(OUTPUT_DIR, exist_ok=True)
            with open(f"{OUTPUT_DIR}/{filename}.txt", "w") as f:
                f.write(f"Processed on {device}")
                
            status = "SUCCESS"
        except Exception as e:
            status = "FAILED"
        
        yield (filename, status)

if __name__ == "__main__":
    # Kh·ªüi t·∫°o Spark (Ch·∫ø ƒë·ªô local[2] cho Kaggle T4 x2)
    spark = SparkSession.builder \
        .appName("ZeroStereo_MapReduce") \
        .config("spark.driver.memory", "14g") \
        .master("local[2]") \
        .getOrCreate()

    print(f"üöÄ [Spark] ƒêang qu√©t ·∫£nh t·ª´: {INPUT_DIR}")
    
    # Ki·ªÉm tra folder ƒë·∫ßu v√†o
    if not os.path.exists(INPUT_DIR):
        print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y ƒë∆∞·ªùng d·∫´n {INPUT_DIR}")
        print("üëâ H√£y ki·ªÉm tra l·∫°i t√™n Dataset trong ph·∫ßn 'Add Input' tr√™n Kaggle!")
        sys.exit(1)

    all_files = glob.glob(os.path.join(INPUT_DIR, "*.jpg"))
    print(f"üìä T·ªïng s·ªë ·∫£nh t√¨m th·∫•y: {len(all_files)}")

    # CH·∫†Y TEST 100 ·∫¢NH
    if len(all_files) > 0:
        files_rdd = spark.sparkContext.parallelize(all_files[:100], numSlices=2)
        
        print("‚è≥ ƒêang ch·∫°y MapReduce...")
        # Map: X·ª≠ l√Ω
        mapped_rdd = files_rdd.mapPartitions(process_partition)
        
        # Reduce: T·ªïng h·ª£p
        summary = mapped_rdd.map(lambda x: (x[1], 1)) \
                            .reduceByKey(lambda a, b: a + b) \
                            .collect()

        print("-" * 40)
        print("‚úÖ K·∫æT QU·∫¢ TH·ªêNG K√ä (REDUCE):")
        for status, count in summary:
            print(f"   - {status}: {count}")
        print("-" * 40)
    
    spark.stop()
import os
import glob
from pyspark.sql import SparkSession
import torch
from diffusers import StableDiffusionInpaintPipeline
# Import cÃ¡c hÃ m cáº§n thiáº¿t tá»« project cá»§a báº¡n
# LÆ°u Ã½: Báº¡n cáº§n copy cÃ¡c file model/, util/, config/ vÃ o cÃ¹ng thÆ° má»¥c cháº¡y Spark
# Hoáº·c Ä‘Ã³ng gÃ³i chÃºng thÃ nh file .zip Ä‘á»ƒ gá»­i kÃ¨m Spark job

# --- Cáº¤U HÃŒNH ---
INPUT_DIR = "D:/Data/bdd100k_dataset/bdd100k/images/100k/train" # ÄÆ°á»ng dáº«n dataset 10GB
OUTPUT_DIR = "D:/Data/output_spark_mapreduce"
CHECKPOINT_DIR = "checkpoints/StereoGen" # ÄÆ°á»ng dáº«n model Ä‘Ã£ táº£i

# HÃ m xá»­ lÃ½ (MAPPER) - Cháº¡y trÃªn tá»«ng Worker node
def process_partition(iterator):
    """
    HÃ m nÃ y nháº­n vÃ o má»™t danh sÃ¡ch cÃ¡c Ä‘Æ°á»ng dáº«n áº£nh (Iterator),
    Load model AI má»™t láº§n duy nháº¥t, sau Ä‘Ã³ xá»­ lÃ½ háº¿t danh sÃ¡ch Ä‘Ã³.
    """
    results = []
    
    # 1. Load Model (Chá»‰ load 1 láº§n trÃªn má»—i Partition Ä‘á»ƒ tiáº¿t kiá»‡m RAM/Time)
    # LÆ°u Ã½: Cáº§n import thÆ° viá»‡n bÃªn trong hÃ m Ä‘á»ƒ trÃ¡nh lá»—i serialize cá»§a Spark
    import torch
    from diffusers import StableDiffusionInpaintPipeline
    
    # Giáº£ láº­p load model (Thay báº±ng code load model tháº­t cá»§a báº¡n trong generate_stereo.py)
    # VÃ¬ Spark cháº¡y Ä‘a luá»“ng, cáº§n cáº©n tháº­n vá»›i CUDA. 
    # Náº¿u cháº¡y local mode, model sáº½ tranh nhau GPU. Tá»‘t nháº¥t set device='cpu' hoáº·c giá»›i háº¡n worker.
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    try:
        # á» Ä‘Ã¢y mÃ¬nh demo load pipeline SD Ä‘Æ¡n giáº£n. 
        # Thá»±c táº¿ báº¡n cáº§n bÃª logic load model tá»« 'generate_stereo.py' vÃ o Ä‘Ã¢y.
        print(f"Worker Ä‘ang load model trÃªn thiáº¿t bá»‹: {device}")
        # model = ... (Load model ZeroStereo cá»§a báº¡n táº¡i Ä‘Ã¢y)
        
        # 2. Láº·p qua tá»«ng áº£nh trong partition Ä‘Æ°á»£c giao
        for image_path in iterator:
            filename = os.path.basename(image_path)
            try:
                # --- LOGIC Xá»¬ LÃ áº¢NH (Inference) ---
                # image = read_image(image_path)
                # result = model(image)
                # save_image(result, OUTPUT_DIR + filename)
                
                # Giáº£ láº­p xá»­ lÃ½ xong
                status = "SUCCESS"
                
            except Exception as e:
                status = f"FAILED: {str(e)}"
            
            # Tráº£ vá» káº¿t quáº£ dáº¡ng Key-Value cho bÆ°á»›c Reduce
            yield (filename, status)
            
    except Exception as e:
        yield ("System_Error", str(e))

# --- CHÆ¯Æ NG TRÃŒNH CHÃNH (DRIVER) ---
if __name__ == "__main__":
    # 1. Khá»Ÿi táº¡o Spark Session
    spark = SparkSession.builder \
        .appName("ZeroStereo_MapReduce") \
        .config("spark.driver.memory", "4g") \
        .master("local[*]") \
        .getOrCreate()
        # local[*] nghÄ©a lÃ  dÃ¹ng táº¥t cáº£ CPU core cá»§a mÃ¡y báº¡n lÃ m worker

    print("ğŸš€ Báº¯t Ä‘áº§u Job MapReduce ZeroStereo...")

    # 2. Äá»c danh sÃ¡ch file (Táº¡o RDD)
    # TÃ¬m táº¥t cáº£ file jpg trong thÆ° má»¥c 10GB
    all_files = glob.glob(os.path.join(INPUT_DIR, "*.jpg"))
    # Chá»‰ láº¥y thá»­ 100 áº£nh Ä‘á»ƒ test trÆ°á»›c khi cháº¡y full
    all_files = all_files[:100] 
    
    # PhÃ¢n tÃ¡n danh sÃ¡ch file vÃ o RDD (Resilient Distributed Dataset)
    # numSlices=4 chia dá»¯ liá»‡u thÃ nh 4 pháº§n cho 4 worker xá»­ lÃ½ song song
    files_rdd = spark.sparkContext.parallelize(all_files, numSlices=4)

    # 3. Giai Ä‘oáº¡n MAP: Xá»­ lÃ½ áº£nh song song
    # mapPartitions hiá»‡u quáº£ hÆ¡n map() vÃ¬ load model 1 láº§n/batch
    mapped_rdd = files_rdd.mapPartitions(process_partition)

    # 4. Giai Ä‘oáº¡n REDUCE: Tá»•ng há»£p káº¿t quáº£
    # Gom cÃ¡c tráº¡ng thÃ¡i láº¡i vÃ  Ä‘áº¿m
    # (Key, Value) -> Äáº¿m sá»‘ lÆ°á»£ng Success/Failed
    summary = mapped_rdd.map(lambda x: (x[1].split(':')[0], 1)) \
                        .reduceByKey(lambda a, b: a + b) \
                        .collect()

    print("-" * 40)
    print("ğŸ“Š Káº¾T QUáº¢ MAP REDUCE:")
    for status, count in summary:
        print(f"   - {status}: {count} áº£nh")
    print("-" * 40)

    spark.stop()